## 集群搭建

### 1、配置SSH免登录

```shell
  $ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
  $ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
  $ chmod 0600 ~/.ssh/authorized_keys
```

拷贝`~/.ssh/authorized_keys`内容至集群各台设备。

### 2、ntp时间同步

ntp.conf

```c
driftfile /var/lib/ntp/drift

restrict default kod nomodify notrap nopeer noquery
restrict -6 default kod nomodify notrap nopeer noquery

restrict 127.0.0.1
restrict -6 ::1

restrict 192.168.245.0 mask 255.255.255.0 nomodify notrap nopeer

# aliyun
server 182.92.12.11 prefer
server 115.28.122.198

includefile /etc/ntp/crypto/pw

keys /etc/ntp/keys
```

```
driftfile /var/lib/ntp/drift

restrict default kod nomodify notrap nopeer noquery
restrict -6 default kod nomodify notrap nopeer noquery

restrict 127.0.0.1 
restrict -6 ::1 

restrict c601
server c601 iburst

includefile /etc/ntp/crypto/pw
```

### 3、Single Node

#### etc/hadoop/core-site.xml:

```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
```

#### etc/hadoop/hdfs-site.xml:

```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>
```

#### 初始化&启动

```shell
 $ bin/hdfs namenode -format
 $ sbin/start-dfs.sh
```

### 4、Cluster （HA QJM）

#### Zookeeper

##### zoo.conf

```properties
# The number of milliseconds of each tick
tickTime=2000                                                                                                                                                                                                    
# The number of ticks that the initial 
# synchronization phase can take
initLimit=10
# The number of ticks that can pass between 
# sending a request and getting an acknowledgement
syncLimit=5
# the directory where the snapshot is stored.
# do not use /tmp for storage, /tmp here is just 
# example sakes.
dataDir=/data/zookeeper/hdfs_federation
# the port at which the clients will connect
clientPort=2181
# the maximum number of client connections.
# increase this if you need to handle more clients
#maxClientCnxns=60
#
# Be sure to read the maintenance section of the 
# administrator guide before turning on autopurge.
#
# http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance
#
# The number of snapshots to retain in dataDir
#autopurge.snapRetainCount=3
# Purge task interval in hours
# Set to "0" to disable auto purge feature
#autopurge.purgeInterval=1
server.1=c601:2888:3888
server.2=c602:2888:3888
server.3=c603:2888:3888
server.4=c604:2888:3888
server.5=c605:2888:3888
```

##### LogDir

bin/zkEnv.sh

```shell
if [ "x${ZOO_LOG_DIR}" = "x" ]
then
    ZOO_LOG_DIR="/data/zookeeper"
fi
```

##### myid

每个节点DataDir下创建myid文件，内容即zoo.conf中配置的该节点ID。

#### 配置

##### core-site.xml

```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://c6cluster/</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <!-- 目录必须为空或不存在 -->
        <value>/data/hadoop/tmp/federation/</value>                                                         
    </property>
</configuration>
```

##### hdfs-site.xml

```xml
<configuration>
    <property>
      <name>dfs.nameservices</name>
      <value>c6cluster</value>
    </property>
    <property>
      <name>dfs.ha.namenodes.c6cluster</name>
      <value>nn1,nn2</value>
    </property>
    <property>
      <name>dfs.namenode.rpc-address.c6cluster.nn1</name>
      <value>c601:8020</value>
    </property>
    <property>
      <name>dfs.namenode.rpc-address.c6cluster.nn2</name>
      <value>c602:8020</value>
    </property>
    <property>
      <name>dfs.namenode.http-address.c6cluster.nn1</name>
      <value>c601:50070</value>
    </property>
    <property>
      <name>dfs.namenode.http-address.c6cluster.nn2</name>
      <value>c602:50070</value>
    </property>
    <property>
      <name>dfs.namenode.shared.edits.dir</name>
      <value>qjournal://c601:8485;c602:8485;c603:8485/c6cluster</value>
    </property>
    <property>
      <name>dfs.client.failover.proxy.provider.c6cluster</name>
      <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>
    <property>
      <name>dfs.ha.fencing.methods</name>
      <value>sshfence</value>
    </property>
    <property>
      <name>dfs.ha.fencing.ssh.private-key-files</name>
      <value>/root/.ssh/id_rsa</value>
    </property>
    <property>
      <name>dfs.journalnode.edits.dir</name>
      <value>/data/hadoop/journal/federation/</value>
    </property>
    <property>
       <name>dfs.ha.automatic-failover.enabled</name>
       <value>true</value>
     </property>
    <property>
       <name>ha.zookeeper.quorum</name>
       <value>c601:2181,c602:2181,c603:2181</value>
     </property>
</configuration>  
```

##### yarn-site.xml

```xml
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
       <name>yarn.resourcemanager.ha.enabled</name>
       <value>true</value>
     </property>
     <property>
       <name>yarn.resourcemanager.cluster-id</name>
       <value>c6clusteryarn</value>
     </property>
     <property>
       <name>yarn.resourcemanager.ha.rm-ids</name>
       <value>rm1,rm2</value>
     </property>
     <property>
       <name>yarn.resourcemanager.hostname.rm1</name>
       <value>c603</value>
     </property>
     <property>
       <name>yarn.resourcemanager.hostname.rm2</name>
       <value>c604</value>
     </property>
     <property>
       <name>yarn.resourcemanager.zk-address</name>
       <value>c601:2181,c602:2181,c603:2181</value>
     </property>
</configuration> 
```

##### mapred-site.xml

```xml
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>
```

##### slaves

```properties
c603
c604
c605
```

#### 操作

- 启动Zookeeper。
- ​
- 格式化ZK。执行`bin/hdfs zkfc -formatZK`。
- 启动JournalNodes。在每台JN节点上，执行`sbin/hadoop-daemon.sh start journalnode`。
- 格式化NameNode。并格式化一台NameNode。`bin/hdfs namenode -format`。
- NameNode同步。启动已格式化的NameNode，`sbin/hadoop-daemon.sh start namenode`。在**另外一台**执行同步，`bin/hdfs namenode -bootstrapStandby`。
- 停止启动的NameNode。`sbin/hadoop-daemon.sh stop namenode`。
- 启动集群。`sbin/start-dfs.sh`

### 5、局域网yum源

本地yum源搭建
mount /dev/cdrom /mnt
修改 etc yum.repo.d CentOS-Base.repo
baseurl=file:///mnt/
gpgcheck=0


局域网的yum源
1、安装nginx
2、修改nginx配置文件  添加修改 location
        location / {
            root   /mnt;
            autoindex on;
        }
3、mount /dev/cdrom /mnt
4、修改 etc yum.repo.d CentOS-Base.repo
baseurl=http://node1
gpgcheck=0


yum clean all
yum makecache